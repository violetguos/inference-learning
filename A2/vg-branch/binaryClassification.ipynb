{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data binary class Loaded\n",
      "-------------------------------\n",
      "y_hat (?, 1)\n",
      "target (?, 1)\n",
      "learningrate =  [0.001]\n",
      "current entropy 1.05466389359\n",
      "epoch  0.0\n",
      "current entropy 0.106287197357\n",
      "epoch  7.0\n",
      "y_hat (?, 1)\n",
      "target (?, 1)\n",
      "learningrate =  0.001\n",
      "current entropy 0.875107337451\n",
      "epoch  0.0\n",
      "current entropy 0.112776184714\n",
      "epoch  7.0\n",
      "train done\n",
      "train done\n",
      "$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\n",
      "(2, 5000)\n",
      "$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\n",
      "(2, 5000)\n",
      "valideRROR 0.143998445519 testerror 0.132588608208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123de4940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#Const def\n",
    "ERROR_MSE =\"mse\"\n",
    "ERROR_ENTROPY = \"entropy\"\n",
    "\n",
    "\n",
    "\n",
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        np.random.shuffle(data)\n",
    "        np.random.shuffle(targets)\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "        self.epoch = int(np.floor(data.shape[0]/self.batch_size)) \n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def random_batch_indices(self):\n",
    "        index1 = self.i * self.batch_size\n",
    "        index2 = (self.i + 1) * self.batch_size\n",
    "        \n",
    "        if self.i == self.epoch:\n",
    "            self.i = 0\n",
    "        else:\n",
    "            self.i +=1\n",
    "\n",
    "        return int(index1), int(index2) \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        \n",
    "        index1, index2 = self.random_batch_indices()\n",
    "        X_batch = self.data[index1:index2]\n",
    "        y_batch = self.targets[index1:index2]\n",
    "        return X_batch, y_batch  \n",
    "\n",
    "\n",
    "class classification:\n",
    "    #def __init__(self):\n",
    "\n",
    "    def __init__(self, _numClass = 1, _numPixel = 784, _regLambda = 0.01, _regLambdaArr = None, _batchSize = 500, \\\n",
    "                 _batchSizeArr = None,\\\n",
    "                 _learningRate = 0.001, _learningRateArr = None, _numIter = 5000, _gd = 0, _err =0, _qn=0):\n",
    "        self.regLambda = _regLambda #_regLambda\n",
    "        #self.regLambda = tf.cast(self.regLambda, dtype = tf.float64)\n",
    "\n",
    "        self.batchSize = _batchSize\n",
    "        self.learningRate = _learningRate\n",
    "        self.numIter = _numIter\n",
    "        self.numPixel = _numPixel\n",
    "        #self.numEpoch =int(np.ceil(trainData.shape[0]/7)) \n",
    "        \n",
    "        #arrays, default to [one value] if arr is None\n",
    "        self.regLambdaArr =(_regLambdaArr, _regLambda)\n",
    "        self.batchSizeArr = (_batchSizeArr, _batchSize)\n",
    "        self.learningRateArr = (_learningRateArr, _learningRate)\n",
    "        \n",
    "        \n",
    "        self.gd =  _gd  #0 for normal GD, 1 for adam \n",
    "        self.err = _err #for type of error to minimize, 1 for default corssEntropy, 0 MSE error\n",
    "        self.qn = _qn #question type, logistic = 1, linear = 0\n",
    "        self.numClass = _numClass #1 for binary, other for num of classes in multiclass\n",
    "        \n",
    "        #optional    \n",
    "        self.crossEntropyArr = []\n",
    "        self.mseLossArr = []\n",
    "\n",
    "    def paramArrInit(self, paramArr, param):\n",
    "        if paramArr is None:\n",
    "            param = [param]\n",
    "        return paramArr\n",
    "        \n",
    "    \n",
    "    def linearMSE(self):\n",
    "        '''\n",
    "        y_hat, target will be fed\n",
    "\n",
    "        shape = [dim by 1] for binary classification\n",
    "        '''\n",
    "        #print(\"y_hat\",  y_hat)\n",
    "        y_hat = tf.placeholder(tf.float64, shape=[None,self.numClass], name='y_hat')\n",
    "        target = tf.placeholder(tf.float64, shape=[None, self.numClass], name='target')\n",
    "    \n",
    "        se_mat = tf.square(tf.subtract(y_hat, target))\n",
    "        #print(\"msemst\", mse_mat)\n",
    "        mse_mat = tf.reduce_mean(se_mat)\n",
    "        loss = tf.reduce_mean(mse_mat)\n",
    "        loss = tf.div(loss, tf.constant(2.0, dtype = tf.float64))\n",
    "\n",
    "        return y_hat, target, loss \n",
    "    \n",
    "    def linearMSENoFeed(self, y_hat, target):\n",
    "        '''\n",
    "        a tensor function, no feed\n",
    "        '''\n",
    "        print(\"y_hat\",  y_hat.shape)\n",
    "        print(\"target\", target.shape)\n",
    "    \n",
    "        y_hat = tf.convert_to_tensor(y_hat)\n",
    "        target = tf.convert_to_tensor(target)\n",
    "        \n",
    "        target = tf.cast(target, dtype = 'float64')\n",
    "        se_mat = tf.square(tf.subtract(y_hat, target))\n",
    "        #print(\"msemst\", mse_mat)\n",
    "        mse_mat = tf.reduce_mean(se_mat)\n",
    "\n",
    "        loss = tf.reduce_mean(mse_mat)\n",
    "\n",
    "        loss = loss/2.0 #tf.div(loss, tf.constant(2.0))\n",
    "        #print(mse_mat.eval())\n",
    "\n",
    "        return loss  \n",
    "    \n",
    "    def crossEntropyError(self, y_hat, y_target):\n",
    "        crossEntropyErrorCurr = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_target, logits= y_hat))\n",
    "        return crossEntropyErrorCurr\n",
    "    \n",
    "    def accuracy(y_hat, target, classType = 0):\n",
    "        if classType == 0:\n",
    "            correctCases = tf.equal(tf.cast(tf.greater_equal(y_hat, 0.5), tf.float64), tf.floor(target))\n",
    "        else:\n",
    "            correctCases = tf.equal(tf.argmax(y_hat, 1), tf.argmax(target, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctCases, dtype=\"float\"))\n",
    "    \n",
    "        return accuracy\n",
    "    \n",
    "    def buildGraph(self):\n",
    "        '''\n",
    "        Input: _data is x in the equation, dim by 784 flattened tensor\n",
    "               _target is y in the equaion\n",
    "               _regLambda is the wegithed decay coeff\n",
    "               _learningRate is the epsilon\n",
    "               err is the type of error, 1 default corssEntropy\n",
    "        '''\n",
    "\n",
    "        #declare using a placeholder, feed in _data and _target to x ,y \n",
    "        X = tf.placeholder(tf.float64, shape=[None, self.numPixel], name='dataX')\n",
    "        \n",
    "        # W initialize to a gaussian distr, honestly anything would work\n",
    "        W = tf.Variable(tf.truncated_normal(shape=[self.numPixel, self.numClass], stddev=0.1), name='weights')\n",
    "\n",
    "        W = tf.cast(W, dtype=tf.float64)\n",
    "        b = tf.Variable(0.0, name='biases')\n",
    "        b = tf.cast(b, dtype=tf.float64)\n",
    "\n",
    "        y_target = tf.placeholder(tf.float64, shape=[None, self.numClass], name='targetY')\n",
    "        \n",
    "\n",
    "        #compute the current y_hat\n",
    "        if self.qn == 1: #logistic:\n",
    "            wtxb =  tf.matmul(X, W) + b\n",
    "            y_hat = tf.sigmoid(wtxb)\n",
    "            regTerm = tf.multiply( tf.constant(0.50, dtype = tf.float64),\\\n",
    "                                  tf.multiply(tf.constant(self.regLambda,dtype = tf.float64), tf.reduce_mean(tf.square(W))))\n",
    "            crossEntropyErrorCurr = self.crossEntropyError(y_hat, y_target)\n",
    "            crossEntropyErrorCurr = tf.add(crossEntropyErrorCurr, regTerm)  \n",
    "            errTerm = crossEntropyErrorCurr\n",
    "            \n",
    "        else:\n",
    "            y_hat =  tf.matmul(X, W) + b\n",
    "            mseCurr = self.linearMSENoFeed(y_hat, y_target)\n",
    "            regTerm = tf.multiply( tf.constant(0.50, dtype = tf.float64), tf.multiply(tf.constant(self.regLambda,dtype = tf.float64), tf.reduce_mean(tf.square(W))))\n",
    "            mseCurr = tf.add(mseCurr, regTerm)\n",
    "            errTerm = mseCurr\n",
    "        \n",
    "\n",
    "        if self.gd == 0:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.learningRate)\n",
    "        else:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "    \n",
    "        if self.err == 1:\n",
    "            train = optimizer.minimize(loss=crossEntropyErrorCurr)\n",
    "        else:\n",
    "            train = optimizer.minimize(loss=mseCurr)\n",
    "\n",
    "        return W, b, errTerm, y_hat, X, y_target, train\n",
    "    \n",
    "    def plotFig(self, _dim, y , addInfo, title=\"default\", xLabel=\"xlabel\", yLabel=\"yLabel\", plotLabel =\"plotLabel\", _num =1 ):\n",
    "        x = np.linspace(0, _dim, num=_dim)\n",
    "        y = np.array(y)\n",
    "        print(\"$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\")\n",
    "        print(y.shape)\n",
    "        plt.figure(_num)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xLabel)\n",
    "        plt.ylabel(yLabel)\n",
    "        for i in range(y.shape[0]):\n",
    "            #print(\"x\", x)\n",
    "            #print(\"y\", y[i])\n",
    "            plt.plot(x, y[i], label = plotLabel + str(addInfo[i]))\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig( title + str(_num) + \".png\")\n",
    "        plt.close()\n",
    "        plt.clf()\n",
    "\n",
    "              \n",
    "    def runLogisticGraphPart1(self, trainData, trainTarget, validData, validTarget,testData, testTarget,\n",
    "                              plotOut):\n",
    "        '''\n",
    "        Input: _data,\n",
    "        Output:\n",
    "           required accuracy/epoch plots\n",
    "           \n",
    "        '''\n",
    "        trainLossArr= []\n",
    "        mseLossArr = []\n",
    "        \n",
    "            \n",
    "        for learningRate in self.learningRateArr:\n",
    "            trainLossL = []\n",
    "            mseLossL = []\n",
    "            tf.reset_default_graph()\n",
    "            W, b, errTerm, y_hat, X, y_target, train = self.buildGraph()\n",
    "            y_hat_mse, target_mse, mseLoss = self.linearMSE()\n",
    "    \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.InteractiveSession()\n",
    "            sess.run(init)\n",
    "            initialW = sess.run(W)  \n",
    "            initialb = sess.run(b)            \n",
    "            #training model and iter through batches\n",
    "            print(\"learningrate = \", learningRate)\n",
    "            trainBatchSampler = BatchSampler(trainData, trainTarget, self.batchSize)\n",
    "        \n",
    "            for i in range(self.numIter):\n",
    "                dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "                currentW, currentb, errTrain, y_predict, trainModel = sess.run([W, b, errTerm, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "                trainLossL.append(errTrain)\n",
    "            \n",
    "                mseLossVal = sess.run(mseLoss, feed_dict={y_hat_mse: y_predict, target_mse: targetBatch})\n",
    "                mseLossL.append(mseLossVal)\n",
    "            \n",
    "                if i%3500 == 0:\n",
    "                    print(\"current entropy\", errTrain)\n",
    "                    #print(\"current mse\", mseLoss)\n",
    "                    print(\"epoch \", i/500)\n",
    "        \n",
    "            trainLossArr.append(trainLossL)\n",
    "            mseLossArr.append(mseLossL)\n",
    "        \n",
    "        print(\"train done\")\n",
    "        #print(\"mselossall\", mseLossAll)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        for i in range(len(plotOut)):\n",
    "            self.plotFig(self.numIter, trainLossArr, self.learningRateArr,  title = plotOut[i],\\\n",
    "            plotLabel=\"learning rate\", _num = i)\n",
    "        '''\n",
    "        \n",
    "        print(\"train done\")\n",
    "        self.plotFig(self.numIter, trainLossArr, self.learningRateArr,  title = \"test1\",\\\n",
    "            plotLabel=\"learning rate\")\n",
    "        self.plotFig(self.numIter, mseLossArr, self.learningRateArr, title = \"test2\",\\\n",
    "           plotLabel = \"learning rate\")\n",
    "    \n",
    "        #test error and valid error\n",
    "        validHat = sess.run(y_hat, feed_dict={X: validData, y_target: validTarget})\n",
    "        testHat = sess.run(y_hat, feed_dict={X: testData, y_target: testTarget} )\n",
    "    \n",
    "    \n",
    "        validError = sess.run(mseLoss, feed_dict={y_hat_mse: validHat, target_mse: validTarget}) \n",
    "        testError =  sess.run(mseLoss, feed_dict={y_hat_mse: testHat, target_mse: testTarget}) \n",
    "\n",
    "        print(\"valideRROR\", validError, \"testerror\", testError)\n",
    "\n",
    "class loadData:\n",
    "    def __init__(self):\n",
    "        self.flatten = True\n",
    "        self.addOnes = False\n",
    "        \n",
    "        self.data_path = '/Users/vikuo/Documents/GitHub/ece521/assi/A1/data/data.npy'\n",
    "        self.target_path = '/Users/vikuo/Documents/GitHub/ece521/assi/A1/data/target.npy'\n",
    "    \n",
    "    def arrFlatten(self, arr):\n",
    "        '''\n",
    "        type np array\n",
    "        '''\n",
    "        dataDim1, dum1, dum2 = arr.shape\n",
    "        dum_sq = dum1 * dum2\n",
    "        arr = np.reshape(arr, [ dataDim1 ,dum_sq ])\n",
    "        return arr   \n",
    "    def convertTarget(self, targetValues):\n",
    "        numClasses = np.max(targetValues) + 1\n",
    "        return np.eye(numClasses)[targetValues]\n",
    "    \n",
    "    def loadBinData(self):\n",
    "    # import binary NOTMIST data set\n",
    "        with np.load(\"notMNIST.npz\") as data :\n",
    "            Data, Target = data [\"images\"], data[\"labels\"]\n",
    "            posClass = 2\n",
    "            negClass = 9\n",
    "            dataIndx = (Target==posClass) + (Target==negClass)\n",
    "            Data = Data[dataIndx]/255.\n",
    "            Target = Target[dataIndx].reshape(-1, 1)\n",
    "            Target[Target==posClass] = 1\n",
    "            Target[Target==negClass] = 0\n",
    "            \n",
    "            if self.flatten:\n",
    "                Data = self.arrFlatten(Data)\n",
    "        \n",
    "            if self.addOnes:\n",
    "                Data = np.concatenate((np.ones((Data.shape[0], 1)),Data),axis=1)\n",
    "        \n",
    "        \n",
    "            np.random.seed(521)\n",
    "            randIndx = np.arange(len(Data))\n",
    "            np.random.shuffle(randIndx)\n",
    "            Data, Target = Data[randIndx], Target[randIndx]\n",
    "            trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "            validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "            testData, testTarget = Data[3600:], Target[3600:]\n",
    "        \n",
    "        print(\"Data binary class Loaded\")\n",
    "        print(\"-------------------------------\")\n",
    "        return trainData, trainTarget,validData, validTarget, testData, testTarget\n",
    "    \n",
    "    \n",
    "    def loadMultiData(self):\n",
    "        with np.load(\"notMNIST.npz\") as data:\n",
    "            Data, Target = data [\"images\"], data[\"labels\"]\n",
    "            np.random.seed(521)\n",
    "            randIndx = np.arange(len(Data))\n",
    "            np.random.shuffle(randIndx)\n",
    "            Data = Data[randIndx]/255.\n",
    "            \n",
    "            if self.flatten:\n",
    "                Data = self.arrFlatten(Data)\n",
    "            \n",
    "            Target = Target[randIndx]\n",
    "            trainData, trainTarget = Data[:15000], Target[:15000]\n",
    "            validData, validTarget = Data[15000:16000], Target[15000:16000]\n",
    "            testData, testTarget = Data[16000:], Target[16000:]\n",
    "        \n",
    "            trainTarget = self.convertTarget(trainTarget)\n",
    "            validTarget = self.convertTarget(validTarget)\n",
    "            testTarget = self.convertTarget(testTarget)\n",
    "        return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "    \n",
    "    \n",
    "    def loadFaceData(self):\n",
    "        task = 0\n",
    "        # task = 0 >> select the name ID targets for face recognition task\n",
    "        # task = 1 >> select the gender ID targets for gender recognition task data = np.load(data_path)/255\n",
    "        data = np.load(self.data_path)/255\n",
    "        data = np.reshape(data, [-1, 32*32])\n",
    "        target = np.load(self.target_path)\n",
    "        np.random.seed(45689)\n",
    "        rnd_idx = np.arange(np.shape(data)[0])\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        trBatch = int(0.8*len(rnd_idx))\n",
    "        validBatch = int(0.1*len(rnd_idx))\n",
    " \n",
    "        trainData, validData, testData = data[rnd_idx[1:trBatch],:], \\\n",
    "                                   data[rnd_idx[trBatch+1:trBatch + validBatch],:],\\\n",
    "                                   data[rnd_idx[trBatch + validBatch+1:-1],:]\n",
    "        trainTarget, validTarget, testTarget = target[rnd_idx[1:trBatch], task], \\\n",
    "                              target[rnd_idx[trBatch+1:trBatch + validBatch], task],\\\n",
    "                              target[rnd_idx[trBatch + validBatch + 1:-1], task]\n",
    "        #print(\"train data dim\", trainData.shape, \"valid data dim\", validData.shape,\n",
    "        #     \"test data dim\", testData.shape, \"trainTarget shape\", trainTarget.shape,\n",
    "        #     \"validTarget SHAPE\", validTarget.shape, \"testTarget shape\", testTarget.shape)\n",
    "        trainTarget = self.convertTarget(trainTarget)\n",
    "        validTarget = self.convertTarget(validTarget)\n",
    "        testTarget = self.convertTarget(testTarget)\n",
    "        return trainData, trainTarget, validData, validTarget,testData, testTarget \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    dataLoader = loadData()\n",
    "    trainData, trainTarget, validData, validTarget,testData, testTarget = dataLoader.loadBinData()\n",
    "    \n",
    "    #trainData, trainTarget, validData, validTarget,testData, testTarget = dataLoader.loadMultiData()\n",
    "    #trainData, trainTarget, validData, validTarget,testData, testTarget  = dataLoader.loadFaceData()\n",
    "    plotTitleArr = [\"test\"]\n",
    "\n",
    "    #plotTitleArr = [\"q2-3 Adam Opt lambda = 0 linear loss vs number of epoches\"]\n",
    "    linearClassifier = classification(_numClass = 1, _numPixel = 784, _regLambda = 0.001, _batchSize = 500, \\\n",
    "                 _learningRate = 0.001, _learningRateArr = [0.001], _numIter = 5000, _gd =1, _err=0, _qn=0)\n",
    "    linearClassifier.runLogisticGraphPart1(trainData, trainTarget, validData, validTarget,testData, testTarget, plotTitleArr)\n",
    "    #plotTitleArr = [\"Q2-1 logistic loss vs number of epoches\", \"Q2-1 MSE loss vs number of epoches\"]\n",
    "    #logisticClassifier = classification(_numClass = 1, _numPixel = 784, _regLambda = 0.001, _batchSize = 500, \\\n",
    "    #             _learningRate = 0.001, _learningRateArr = [0.001], _numIter = 5000, _gd =1, _err=1, _qn=1)\n",
    "    #logisticClassifier.runLogisticGraphPart1(trainData, trainTarget, validData, validTarget,testData, testTarget, plotTitleArr)\n",
    "\n",
    "    #multiLogistic = classification(_numClass = 10,_numPixel = 784, _regLambda = 0.01, _batchSize = 500, \\\n",
    "    #            _learningRate = 0.001, _learningRateArr = [0.005, 0.001, 0.0001], _numIter = 5000, _gd =1, _err=1, _qn=1)\n",
    "    #multiLogistic.runLogisticGraphPart1(trainData, trainTarget, validData, validTarget,\\\n",
    "    #                                    testData, testTarget, plotTitleArr)\n",
    "    \n",
    "    #faceLogistic = classification(_numClass = 6, _numPixel = 1024, _regLambda = 0.01, _batchSize = 300, \\\n",
    "    #      _learningRate = 0.001, _learningRateArr = [0.005, 0.001, 0.0001], _numIter = 5, _gd =1, _err=1, _qn=1)\n",
    "    #faceLogistic.runLogisticGraphPart1(trainData, trainTarget, validData, validTarget,testData, testTarget, \\\n",
    "    #                                  plotTitleArr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- [ ] CANNOT reproduce multi minist valid/test errors\n",
    "- [ ] plot accuracies\n",
    "- [ ] organize the work flow (each question plot different things)\n",
    "- [x] change sampler to epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
