{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "        If m is given the batch will be of size m. \n",
    "        Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFig(_num, _dim, y , addInfo, title=\"default\", xLabel=\"xlabel\", yLabel=\"yLabel\", plotLabel =\"plotLabel\" ):\n",
    "    x = np.linspace(0, _dim, num=_dim)\n",
    "   \n",
    "    \n",
    "    y = np.array(y)\n",
    "    print(y.shape)\n",
    "    plt.figure(_num)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.ylabel(yLabel)\n",
    "    for i in range(y.shape[0]):\n",
    "        plt.plot(x, y[i], label = plotLabel + str(addInfo[i]))\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.savefig( title + str(_num) + \".png\")\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3745, 28, 28)\n",
      "Data binary class Loaded\n",
      "-------------------------------\n",
      "*********START Q2 PART 3 LINEAR*******\n",
      "y_hat (?, 1)\n",
      "target (?, 1)\n",
      "mseerRor 1 Tensor(\"truediv:0\", shape=(), dtype=float64)\n",
      "regTerm Tensor(\"Mul_1:0\", shape=(), dtype=float64)\n",
      "mseerRor Tensor(\"Add:0\", shape=(), dtype=float64)\n",
      "learningrate =  0.001\n",
      "current err 0.477470257675\n",
      "epoch  0.0\n",
      "current err 0.0104073910106\n",
      "epoch  1.0\n",
      "train done\n",
      "(1, 5000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d139e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def arrFlatten(arr):\n",
    "    \n",
    "    print(arr.shape)\n",
    "\n",
    "    dataDim1, dum1, dum2 = arr.shape\n",
    "    arr = np.reshape(arr, [ dataDim1 ,784 ])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def loadBinData(linEqn = False):\n",
    "# import binary NOTMIST data set\n",
    "    with np.load(\"notMNIST.npz\") as data :\n",
    "        Data, Target = data [\"images\"], data[\"labels\"]\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        \n",
    "        Data = arrFlatten(Data)\n",
    "        \n",
    "        if linEqn:\n",
    "            Data = np.concatenate((np.ones((Data.shape[0], 1)),Data),axis=1)\n",
    "        \n",
    "        \n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "        \n",
    "    print(\"Data binary class Loaded\")\n",
    "    print(\"-------------------------------\")\n",
    "    return trainData, trainTarget,validData, validTarget,\\\n",
    "        testData, testTarget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linearMSE(y_hat, target):\n",
    "    '''\n",
    "    y_hat, target will be fed\n",
    "    '''\n",
    "    print(\"y_hat\",  y_hat.shape)\n",
    "    print(\"target\", target.shape)\n",
    "    \n",
    "    y_hat = tf.convert_to_tensor(y_hat)\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    \n",
    "    target = tf.cast(target, dtype = 'float64')\n",
    "    se_mat = tf.square(tf.subtract(y_hat, target))\n",
    "    #print(\"msemst\", mse_mat)\n",
    "    mse_mat = tf.reduce_mean(se_mat)\n",
    "\n",
    "    loss = tf.reduce_mean(mse_mat)\n",
    "\n",
    "    loss = loss/2.0 #tf.div(loss, tf.constant(2.0))\n",
    "    #print(mse_mat.eval())\n",
    "\n",
    "    return loss  \n",
    "\n",
    "def accuracy(y_hat, target):\n",
    "    correctCases = tf.equal(tf.cast(tf.greater_equal(y_hat, 0.5), tf.float64), tf.floor(target))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctCases, dtype=\"float\"))\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "def fit_regression(X,Y,  validX, validY, testX, testY):\n",
    "    #TODO: implement linear regression\n",
    "    # Remember to use np.linalg.solve instead of inverting!\n",
    "    #X = np.concatenate((np.ones((X.shape[0],1)),X),axis=1) #add constant one feature - no bias needed\n",
    "    xtx = np.dot(np.transpose(X), X)\n",
    "    xty = np.dot(np.transpose(X), Y)\n",
    "    w = np.linalg.solve(xtx, xty)\n",
    "    #w = np.linalg.inv(xtx, xty)\n",
    "    \n",
    "    testYhat = np.dot(testX, w)\n",
    "    validYhat = np.dot(validX, w)\n",
    "    #print(testYhat)\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    validErr = linearMSE(validYhat, validY)\n",
    "    testErr = linearMSE(testYhat, testY)\n",
    "    \n",
    "    print(\"validErr\", validErr.eval(), \"testErr\", testErr.eval())\n",
    "\n",
    "\n",
    "\n",
    "def linearNormalEqn(trainData, trainTarget):\n",
    "    y_target = tf.cast(trainTarget, dtype='float32')\n",
    "    \n",
    "    onesX = tf.ones(shape=tf.stack([tf.shape(trainData)[0], 1]))\n",
    "    appendOnesX = tf.concat([trainData, onesX], 1)\n",
    "    w_star = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(tf.transpose(appendOnesX),\\\n",
    "                                                appendOnesX)), tf.transpose(appendOnesX)), y_target)\n",
    "    \n",
    "    print(w_star)\n",
    "    pred_y = tf.matmul(appendOnesX, w_star)\n",
    "    print(pred_y)\n",
    "    y_hat = tf.cast(tf.greater_equal(pred_y, 0.5), tf.float32) #float\n",
    "    print(\"######################\")\n",
    "    print(\"linear normal equation\")\n",
    "    mseError = linearMSE(y_hat, trainTarget)\n",
    "    print(\"mse error\", mseError)\n",
    "    \n",
    "    return mseError\n",
    "'''\n",
    "def linearNormalEqn(trainData, trainTarget):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 784], name='dataX')\n",
    "    b = tf.Variable(0.0, name='biases')\n",
    "    y_target = tf.placeholder(tf.float32, shape=[None, 1], name='targetY')\n",
    "    \n",
    "    onesX = tf.ones(shape=(1, 784))   #(shape=tf.stack([tf.shape(X)[0], 1]))\n",
    "    appendOnesX = tf.concat([X, onesX], 0) #1 for offset b\n",
    "    print(appendOnesX)\n",
    "    w_star = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(tf.transpose(appendOnesX),\\\n",
    "                                                appendOnesX)), tf.transpose(appendOnesX)), y_target)\n",
    "\n",
    "    print(w_star)\n",
    "    pred_y = tf.matmul(appendOnesX, w_star)\n",
    "    print(pred_y)\n",
    "    y_hat = tf.cast(tf.greater_equal(pred_y, 0.5), tf.float32) #float to bool to float 1 or 0\n",
    "    print(y_hat)\n",
    "    mseError = linearMSE(y_hat, trainTarget)\n",
    "    \n",
    "    \n",
    "    #finish build graph\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    errTrain = sess.run([mseError], feed_dict={X: trainData, y_target: trainTarget})\n",
    "\n",
    "    \n",
    "    print(\"######################\")\n",
    "    print(\"linear normal equation\")\n",
    "    print(\"mse error\", mse)\n",
    "    return X, y_target, errTrain\n",
    "\n",
    "\n",
    "'''\n",
    "def linearBuildGraph(_regLambda, _learningRate, gd = True):\n",
    "    '''\n",
    "    Input: _data is x in the equation, dim by 784 flattened tensor\n",
    "       _target is y in the equaion\n",
    "       _regLambda is the wegithed decay coeff\n",
    "       _learningRate is the epsilon\n",
    "    '''\n",
    "    _regLambda = tf.cast(_regLambda, dtype = tf.float64)\n",
    "\n",
    "    #declare using a placeholder, feed in _data and _target to x ,y \n",
    "    #x_dim, dum1 =_data.get_shape().as_list()\n",
    "    X = tf.placeholder(tf.float64, shape=[None, 784], name='dataX')\n",
    "    # W initialize to a gaussian distr, honestly anything would work\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[784, 1], stddev=0.1), name='weights')\n",
    "    W = tf.cast(W, dtype=tf.float64)\n",
    "    b = tf.Variable(0.0, name='biases')\n",
    "    b = tf.cast(b, dtype=tf.float64)\n",
    "\n",
    "    y_target = tf.placeholder(tf.float64, shape=[None, 1], name='targetY')\n",
    "    \n",
    "    #compute the current y_hat\n",
    "    y_hat =  tf.matmul(X, W) + b\n",
    "    #compute the current loss\n",
    "    \n",
    "    \n",
    "    mseCurr = linearMSE(y_hat, y_target)\n",
    "    print(\"mseerRor 1\", mseCurr)\n",
    "\n",
    "    #compute the decay/regularization term\n",
    "    regTerm =tf.multiply(tf.constant(0.50, dtype = tf.float64), tf.multiply(_regLambda, tf.reduce_mean(tf.square(W))))\n",
    "    \n",
    "    print(\"regTerm\", regTerm)\n",
    "    mseCurr = tf.add(mseCurr, regTerm)\n",
    "    print(\"mseerRor\", mseCurr)\n",
    "    \n",
    "    \n",
    "    if gd:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = _learningRate)\n",
    "    else:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = _learningRate)\n",
    "    \n",
    "    train = optimizer.minimize(loss=mseCurr)\n",
    "\n",
    "    return W, b, mseCurr, y_hat, X, y_target, train\n",
    "    \n",
    "\n",
    "def runLinearGraphPart1(trainData, trainTarget ):\n",
    "    \n",
    "    '''\n",
    "    Input: _data,\n",
    "           _target,\n",
    "           _numIters\n",
    "    Output:\n",
    "           required accuracy/epoch plots\n",
    "           \n",
    "    '''\n",
    "    \n",
    "    regLambda = 0.0\n",
    "    learningRateArr = [0.005, 0.001, 0.0001]\n",
    "    numIter = 20000\n",
    "    numEpoch =int(np.ceil(20000/7))\n",
    "    batchSize = 500\n",
    "    epochTrainSize = 3500\n",
    "    trainLossAll = []\n",
    "    \n",
    "    for learningRate in learningRateArr:\n",
    "        trainLossLR = []\n",
    "        tf.reset_default_graph()\n",
    "        W, b, mseError, y_hat, X, y_target, train = linearBuildGraph(regLambda, learningRate)\n",
    "    \n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(init)\n",
    "        initialW = sess.run(W)  \n",
    "        initialb = sess.run(b)            \n",
    "        #training model and iter through batches\n",
    "        print(\"learningrate = \", learningRate)\n",
    "        \n",
    "        for i in range(numIter):\n",
    "            trainBatchSampler = BatchSampler(trainData, trainTarget, batchSize)\n",
    "            dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "            #dataBatch = tf.stack(dataBatch)\n",
    "            #targetBatch = tf.stack(targetBatch)\n",
    "            currentW, currentb, errTrain, y_predict, trainModel = sess.run([W, b, mseError, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "            trainLossLR.append(errTrain)\n",
    "            if i%3500 == 0:\n",
    "                print(\"current err\", errTrain)\n",
    "                print(\"epoch \", i/3500)\n",
    "        trainLossAll.append(trainLossLR)\n",
    "        \n",
    "    print(\"train done\")\n",
    "    plotFig(1, numIter, trainLossAll, learningRateArr,  title = \"loss vs number of epoches\",\\\n",
    "            plotLabel=\"learning rate\")\n",
    "    \n",
    "    \n",
    "def runLinearGraphPart2(trainData, trainTarget ):\n",
    "    \n",
    "    '''\n",
    "    Input: _data,\n",
    "           _target,\n",
    "           _numIters\n",
    "    Output:required accuracy/epoch plots\n",
    "           \n",
    "           \n",
    "    '''\n",
    "    \n",
    "    regLambda = 0.0\n",
    "    learningRate = 0.005 #chosen from part1\n",
    "    numIter = 20000\n",
    "    numEpoch =int(np.ceil(20000/7))\n",
    "    batchSizeArr = [500, 1500, 3500]\n",
    "    epochTrainSize = 3500\n",
    "    trainLossAll = []\n",
    "    \n",
    "    for batchSize in batchSizeArr:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trainLossLR = []\n",
    "        tf.reset_default_graph()\n",
    "        W, b, mseError, y_hat, X, y_target, train = linearBuildGraph(regLambda, learningRate)\n",
    "    \n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(init)\n",
    "        initialW = sess.run(W)  \n",
    "        initialb = sess.run(b)\n",
    "        \n",
    "        print(\"batchSize\", batchSize)\n",
    "        for i in range(numIter):\n",
    "            trainBatchSampler = BatchSampler(trainData, trainTarget, batchSize)\n",
    "            dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "            #dataBatch = tf.stack(dataBatch)\n",
    "            #targetBatch = tf.stack(targetBatch)\n",
    "            currentW, currentb, errTrain, y_predict, trainModel = sess.run([W, b, mseError, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "            #trainLossLR.append(errTrain)\n",
    "            if i%3500 == 0:\n",
    "                print(\"current err\", errTrain)\n",
    "                print(\"epoch \", i/3500)\n",
    "        \n",
    "        trainLossAll.append(errTrain)\n",
    "        end = time.time()\n",
    "        elapsed = end - start_time\n",
    "        print(\"time \", elapsed, \"batchsize\", batchSize )\n",
    "    print(\"trainLosssAll\", trainLossAll)    \n",
    "\n",
    "    \n",
    "def runLinearGraphPart3(trainData, trainTarget, validData, validTarget,testData, testTarget):\n",
    "    \n",
    "    '''\n",
    "    Input: _data,\n",
    "           _target,\n",
    "           _numIters\n",
    "    Output:\n",
    "           Lambda results\n",
    "           \n",
    "    '''\n",
    "    \n",
    "    regLambdaArr = [0.0, 0.001, 0.1, 1.0]\n",
    "    learningRate = 0.005\n",
    "    numIter = 20000\n",
    "    numEpoch =int(np.ceil(20000/7))\n",
    "    batchSize = 500\n",
    "    epochTrainSize = 3500\n",
    "    errTestAll = []\n",
    "    errValidAll = []\n",
    "    \n",
    "    for regLambda in regLambdaArr:\n",
    "        errValidArr = []\n",
    "        errTestArr = []\n",
    "        tf.reset_default_graph()\n",
    "        W, b, mseError, y_hat, X, y_target, train = linearBuildGraph(regLambda, learningRate)\n",
    "    \n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(init)\n",
    "        initialW = sess.run(W)  \n",
    "        initialb = sess.run(b)            \n",
    "        #training model and iter through batches\n",
    "        print(\"learningrate = \", learningRate)\n",
    "        for i in range(numIter):\n",
    "            trainBatchSampler = BatchSampler(trainData, trainTarget, batchSize)\n",
    "            dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "            #dataBatch = tf.stack(dataBatch)\n",
    "            #targetBatch = tf.stack(targetBatch)\n",
    "            currentW, currentb, errTrain, y_predict, trainModel = sess.run([W, b, mseError, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "            if i%3500 == 0:\n",
    "                print(\"current err\", errTrain)\n",
    "                print(\"epoch \", i/3500)\n",
    "        \n",
    "            errValid = sess.run(mseError, feed_dict={X: validData, y_target: validTarget})\n",
    "            errTest = sess.run(mseError, feed_dict={X: testData, y_target: testTarget})\n",
    "            errValidArr.append(errValid)\n",
    "            errTestArr.append(errTest)\n",
    "        \n",
    "        errValidAll.append(errValidArr)\n",
    "        errTestAll.append(errTestArr)\n",
    "    \n",
    "    errValidAll = np.array(errValidAll)\n",
    "    errTestAll = np.array(errTestAll)\n",
    "    #print(errValidAll)\n",
    "    for i in range(errValidAll.shape[0]):\n",
    "        best = np.amin(errValidAll[i])\n",
    "        print(\"bset err valid \",best, \"lambda\", regLambdaArr[i])\n",
    "    \n",
    "    for i in range(errTestAll.shape[0]):\n",
    "        best = np.amin(errTestAll[i])\n",
    "        print(\"bset err test \",best, \"lambda\", regLambdaArr[i])\n",
    "    \n",
    "\n",
    "\n",
    "def runQ2Part3Linear(trainData, trainTarget, validData, validTarget,testData, testTarget):\n",
    "    '''\n",
    "    labmda = 0\n",
    "    n = 0.001\n",
    "    adam optimizer for linear\n",
    "    '''\n",
    "    regLambda = 0.0\n",
    "    learningRate = 0.001\n",
    "    learningRateArr = [0.001] #just for plotting \n",
    "\n",
    "    numIter = 5000\n",
    "    numEpoch =int(np.ceil(20000/7))\n",
    "    batchSize = 500\n",
    "    epochTrainSize = 3500 #for training data\n",
    "    trainLossAll = []\n",
    "\n",
    "    trainLossLR = []\n",
    "    tf.reset_default_graph()\n",
    "    W, b, crossEntropyErrorCurr, y_hat, X, y_target, train = linearBuildGraph(regLambda, learningRate, False)\n",
    "    #y_hat_mse, target_mse, mseLoss = linearMSE()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    initialW = sess.run(W)  \n",
    "    initialb = sess.run(b)            \n",
    "    #training model and iter through batches\n",
    "    print(\"learningrate = \", learningRate)\n",
    "    trainBatchSampler = BatchSampler(trainData, trainTarget, batchSize)\n",
    "\n",
    "    for i in range(numIter):\n",
    "        dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "        #dataBatch = tf.stack(dataBatch)\n",
    "        #targetBatch = tf.stack(targetBatch)\n",
    "        currentW, currentb, errTrain, y_predict, trainModel = sess.run([W, b, crossEntropyErrorCurr, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "        trainLossLR.append(errTrain)\n",
    "        #mseLoss = linearMSE()\n",
    "        if i%3500 == 0:\n",
    "            print(\"current err\", errTrain)\n",
    "            print(\"epoch \", i/3500)\n",
    "        \n",
    "    trainLossAll.append(trainLossLR)\n",
    "    \n",
    "    print(\"train done\")\n",
    "    plotFig(4, numIter, trainLossAll, learningRateArr,  title = \"q2-3 Adam Opt lambda = 0 linear loss vs number of epoches\",\\\n",
    "            plotLabel=\"learning rate\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':  \n",
    "        \n",
    "\n",
    "    #trainTarget = arrFlatten(trainTarget)\n",
    "    trainData, trainTarget, validData, validTarget,testData, testTarget = loadBinData(False)\n",
    "    \n",
    "    \n",
    "    #data is (3500, 28, 28)\n",
    "    #the label [1] or [0] is stored in target\n",
    "\n",
    "    #fit_regression(trainData, trainTarget, validData, validTarget,testData, testTarget)\n",
    "\n",
    "    '''\n",
    "    trainData = arrFlatten(trainData)\n",
    "    validData = arrFlatten(validData)\n",
    "    testData = arrFlatten(testData)\n",
    "    print(\"****** START Q1.1 *****\")\n",
    "    runLinearGraphPart1(trainData, trainTarget)\n",
    "    print(\"****** START Q1.2 *****\")\n",
    "    runLinearGraphPart2(trainData, trainTarget)\n",
    "    runLinearGraphPart3(trainData, trainTarget, validData, validTarget,testData, testTarget )\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"*********START Q2 PART 3 LINEAR*******\")\n",
    "    runQ2Part3Linear(trainData, trainTarget, validData, validTarget,testData, testTarget)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time  28.17647886276245 batchsize 500\n",
    "time  142.55758595466614 batchsize 1500\n",
    "time  341.4439833164215 batchsize 3500\n",
    "trainLosssAll [0.013990995, 0.015074042, 0.014701741]\n",
    "\n",
    "bset err valid  0.0174993 lambda 0.0\n",
    "bset err valid  0.0200263 lambda 0.001\n",
    "bset err valid  0.019381 lambda 0.1\n",
    "bset err valid  0.0181959 lambda 1.0\n",
    "bset err test  0.0212588 lambda 0.0\n",
    "bset err test  0.0222249 lambda 0.001\n",
    "bset err test  0.0217733 lambda 0.1\n",
    "bset err test  0.0242665 lambda 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
