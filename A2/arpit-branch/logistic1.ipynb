{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use temporarily until tf.train bug is fixed \n",
    "#violet's mini batch code\n",
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "        If m is given the batch will be of size m. \n",
    "        Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "with np.load(\"notMNIST.npz\") as data :\n",
    "    Data, Target = data [\"images\"], data[\"labels\"]\n",
    "    posClass = 2\n",
    "    negClass = 9\n",
    "    dataIndx = (Target==posClass) + (Target==negClass)\n",
    "    Data = Data[dataIndx]/255.\n",
    "    Target = Target[dataIndx].reshape(-1, 1)\n",
    "    Target[Target==posClass] = 1\n",
    "    Target[Target==negClass] = 0\n",
    "    np.random.seed(521)\n",
    "    randIndx = np.arange(len(Data))\n",
    "    np.random.shuffle(randIndx)\n",
    "    Data, Target = Data[randIndx], Target[randIndx]\n",
    "    trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "    validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "    testData, testTarget = Data[3600:], Target[3600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = np.reshape(trainData, [trainData.shape[0], 28*28])\n",
    "validData = np.reshape(validData, [validData.shape[0], 28*28])\n",
    "testData = np.reshape(testData, [testData.shape[0], 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0.0  -> curr loss: 0.767581\n",
      "epoch: 1.0  -> curr loss: 0.559369\n",
      "epoch: 2.0  -> curr loss: 0.523992\n",
      "epoch: 3.0  -> curr loss: 0.539989\n",
      "epoch: 4.0  -> curr loss: 0.514862\n",
      "epoch: 5.0  -> curr loss: 0.50324\n",
      "epoch: 6.0  -> curr loss: 0.508726\n",
      "epoch: 7.0  -> curr loss: 0.521803\n",
      "epoch: 8.0  -> curr loss: 0.520897\n",
      "epoch: 9.0  -> curr loss: 0.51176\n",
      "epoch: 0.0  -> curr loss: 0.666022\n",
      "epoch: 1.0  -> curr loss: 0.590496\n",
      "epoch: 2.0  -> curr loss: 0.55952\n",
      "epoch: 3.0  -> curr loss: 0.546446\n",
      "epoch: 4.0  -> curr loss: 0.549218\n",
      "epoch: 5.0  -> curr loss: 0.552681\n",
      "epoch: 6.0  -> curr loss: 0.544622\n",
      "epoch: 7.0  -> curr loss: 0.568074\n",
      "epoch: 8.0  -> curr loss: 0.532696\n",
      "epoch: 9.0  -> curr loss: 0.536222\n",
      "epoch: 0.0  -> curr loss: 0.756261\n",
      "epoch: 1.0  -> curr loss: 0.731912\n",
      "epoch: 2.0  -> curr loss: 0.713683\n",
      "epoch: 3.0  -> curr loss: 0.709894\n",
      "epoch: 4.0  -> curr loss: 0.698312\n",
      "epoch: 5.0  -> curr loss: 0.676966\n",
      "epoch: 6.0  -> curr loss: 0.664082\n",
      "epoch: 7.0  -> curr loss: 0.660902\n",
      "epoch: 8.0  -> curr loss: 0.658015\n",
      "epoch: 9.0  -> curr loss: 0.663528\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "\n",
    "learning_rates = [0.005, 0.001, 0.0001]\n",
    "# learning_rates = [0.001]\n",
    "mini_batch = 500\n",
    "reg_coeff = 0.01\n",
    "train_iter = 5000\n",
    "train_loss = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    train_loss_curr = []\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "    y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([784, 1], mean = 0, stddev = 0.1))\n",
    "    \n",
    "    b = tf.cast(tf.Variable(0), dtype = tf.float32)\n",
    "    y_hat = tf.add(tf.matmul(x, w), b)\n",
    "    yhat = tf.sigmoid(y_hat)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = yhat))\n",
    "    \n",
    "    regul = tf.multiply(reg_coeff, tf.reduce_mean(tf.square(w)))\n",
    "    regul = tf.multiply(0.50, regul)\n",
    "    cost = tf.add(cross_entropy, regul)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(train_iter):\n",
    "#         x_batch, y_batch = tf.train.batch([trainData, trainTarget], mini_batch)\n",
    "        \n",
    "        trainBatchSampler = BatchSampler(trainData, trainTarget, mini_batch)\n",
    "        x_batch, y_batch = trainBatchSampler.get_batch()\n",
    "        \n",
    "        sess.run(optimizer, feed_dict = {x: x_batch, y:y_batch})\n",
    "        w_curr, b_curr, c = sess.run([w, b, cost], feed_dict = {x: x_batch, y:y_batch})\n",
    "        train_loss_curr.append(c)\n",
    "        \n",
    "        if epoch%500 is 0:\n",
    "            print(\"epoch:\", epoch/500, \" -> curr loss:\", c)\n",
    "            \n",
    "    train_loss.append(train_loss_curr)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotx = np.linspace(0, train_iter, train_iter)\n",
    "ploty = np.array(train_loss)\n",
    "\n",
    "plt.figure(1)\n",
    "for i in range(ploty.shape[0]):\n",
    "    plt.plot(plotx, ploty[i])\n",
    "\n",
    "plt.savefig(\"logistic_ques1.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0.0  -> curr loss: 0.694329\n",
      "epoch: 1.0  -> curr loss: 0.545058\n",
      "epoch: 2.0  -> curr loss: 0.526127\n",
      "epoch: 3.0  -> curr loss: 0.527051\n",
      "epoch: 4.0  -> curr loss: 0.513873\n",
      "epoch: 5.0  -> curr loss: 0.518628\n",
      "epoch: 6.0  -> curr loss: 0.509437\n",
      "epoch: 7.0  -> curr loss: 0.532809\n",
      "epoch: 8.0  -> curr loss: 0.528536\n",
      "epoch: 9.0  -> curr loss: 0.537141\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "\n",
    "learning_rates = 0.001\n",
    "# learning_rates = [0.001]\n",
    "mini_batch = 500\n",
    "reg_coeff = 0.01\n",
    "np.random.seed(2)\n",
    "train_iter = 5000\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_loss_curr = []\n",
    "    \n",
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "    \n",
    "w = tf.Variable(tf.random_normal([784, 1], mean = 0, stddev = 0.1))\n",
    "    \n",
    "b = tf.cast(tf.Variable(0), dtype = tf.float32)\n",
    "y_hat = tf.add(tf.matmul(x, w), b)\n",
    "yhat = tf.sigmoid(y_hat)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = yhat)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "regul = tf.multiply(reg_coeff, tf.reduce_mean(tf.square(w)))\n",
    "regul = tf.multiply(0.50, regul)\n",
    "cost = tf.add(cross_entropy, regul)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "    \n",
    "for epoch in range(train_iter):\n",
    "#     x_batch, y_batch = tf.train.batch([trainData, trainTarget], mini_batch)\n",
    "        \n",
    "    trainBatchSampler = BatchSampler(trainData, trainTarget, mini_batch)\n",
    "    x_batch, y_batch = trainBatchSampler.get_batch()\n",
    "        \n",
    "    sess.run(optimizer, feed_dict = {x: x_batch, y:y_batch})\n",
    "    w_curr, b_curr, c = sess.run([w, b, cost], feed_dict = {x: x_batch, y:y_batch})\n",
    "    train_loss_curr.append(c)\n",
    "        \n",
    "    if epoch%500 is 0:\n",
    "        print(\"epoch:\", epoch/500, \" -> curr loss:\", c)\n",
    "            \n",
    "train_loss.append(train_loss_curr)\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotx = np.linspace(0, train_iter, train_iter)\n",
    "ploty = np.array(train_loss)\n",
    "\n",
    "plt.figure(1)\n",
    "for i in range(ploty.shape[0]):\n",
    "    plt.plot(plotx, ploty[i])\n",
    "\n",
    "plt.savefig(\"logistic_ques2.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
