{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******q1 part 1********\n",
      "init layer [1000, 10]\n",
      "W 784 1000\n",
      "X shape [None, 784]\n",
      "X in layer Tensor(\"add:0\", shape=(?, 1000), dtype=float32)\n",
      "X_curr Tensor(\"add:0\", shape=(?, 1000), dtype=float32)\n",
      "W 1000 10\n",
      "X shape [None, 1000]\n",
      "X in layer Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "X_curr Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
      "regTermsum Tensor(\"Cast_3:0\", shape=(), dtype=float32)\n",
      "crossEntropyError Curr Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "self.learningrate =  0.001\n",
      "self.epochIter 20\n",
      "currEpoch 0\n",
      "currEpoch 1\n",
      "currEpoch 2\n",
      "currEpoch 3\n",
      "currEpoch 4\n",
      "currEpoch 5\n",
      "currEpoch 5 classErrTerm 0.046 errTrain 0.177497\n",
      "currEpoch 5 classErrTerm 0.072 errTrain 0.258565\n",
      "currEpoch 5 classErrTerm 0.072 errTrain 0.245216\n",
      "currEpoch 5 classErrTerm 0.054 errTrain 0.214313\n",
      "currEpoch 5 classErrTerm 0.054 errTrain 0.234634\n",
      "currEpoch 5 classErrTerm 0.076 errTrain 0.284549\n",
      "currEpoch 5 classErrTerm 0.052 errTrain 0.203569\n",
      "currEpoch 5 classErrTerm 0.054 errTrain 0.198378\n",
      "currEpoch 5 classErrTerm 0.066 errTrain 0.207916\n",
      "currEpoch 5 classErrTerm 0.05 errTrain 0.192551\n",
      "currEpoch 5 classErrTerm 0.082 errTrain 0.271729\n",
      "currEpoch 5 classErrTerm 0.062 errTrain 0.212015\n",
      "currEpoch 5 classErrTerm 0.058 errTrain 0.231094\n",
      "currEpoch 5 classErrTerm 0.058 errTrain 0.226764\n",
      "currEpoch 5 classErrTerm 0.068 errTrain 0.242995\n",
      "currEpoch 5 classErrTerm 0.05 errTrain 0.217481\n",
      "currEpoch 5 classErrTerm 0.058 errTrain 0.235144\n",
      "currEpoch 5 classErrTerm 0.064 errTrain 0.276461\n",
      "currEpoch 5 classErrTerm 0.056 errTrain 0.215314\n",
      "currEpoch 5 classErrTerm 0.056 errTrain 0.221289\n",
      "currEpoch 5 classErrTerm 0.076 errTrain 0.244026\n",
      "currEpoch 5 classErrTerm 0.042 errTrain 0.169477\n",
      "currEpoch 5 classErrTerm 0.046 errTrain 0.153174\n",
      "currEpoch 5 classErrTerm 0.04 errTrain 0.192371\n",
      "currEpoch 5 classErrTerm 0.052 errTrain 0.199312\n",
      "currEpoch 5 classErrTerm 0.058 errTrain 0.201484\n",
      "currEpoch 5 classErrTerm 0.052 errTrain 0.199807\n",
      "currEpoch 5 classErrTerm 0.052 errTrain 0.203722\n",
      "currEpoch 5 classErrTerm 0.06 errTrain 0.228022\n",
      "currEpoch 5 classErrTerm 0.056 errTrain 0.201504\n",
      "currEpoch 6\n",
      "currEpoch 7\n",
      "currEpoch 8\n",
      "currEpoch 9\n",
      "currEpoch 10\n",
      "currEpoch 10 classErrTerm 0.038 errTrain 0.126683\n",
      "currEpoch 10 classErrTerm 0.046 errTrain 0.1725\n",
      "currEpoch 10 classErrTerm 0.032 errTrain 0.119044\n",
      "currEpoch 10 classErrTerm 0.04 errTrain 0.147156\n",
      "currEpoch 10 classErrTerm 0.026 errTrain 0.0976428\n",
      "currEpoch 10 classErrTerm 0.03 errTrain 0.10632\n",
      "currEpoch 10 classErrTerm 0.04 errTrain 0.135724\n",
      "currEpoch 10 classErrTerm 0.024 errTrain 0.0816691\n",
      "currEpoch 10 classErrTerm 0.018 errTrain 0.0916533\n",
      "currEpoch 10 classErrTerm 0.032 errTrain 0.118127\n",
      "currEpoch 10 classErrTerm 0.032 errTrain 0.113627\n",
      "currEpoch 10 classErrTerm 0.016 errTrain 0.107051\n",
      "currEpoch 10 classErrTerm 0.04 errTrain 0.138446\n",
      "currEpoch 10 classErrTerm 0.022 errTrain 0.0977634\n",
      "currEpoch 10 classErrTerm 0.03 errTrain 0.118825\n",
      "currEpoch 10 classErrTerm 0.028 errTrain 0.129212\n",
      "currEpoch 10 classErrTerm 0.022 errTrain 0.106911\n",
      "currEpoch 10 classErrTerm 0.036 errTrain 0.112728\n",
      "currEpoch 10 classErrTerm 0.028 errTrain 0.12039\n",
      "currEpoch 10 classErrTerm 0.03 errTrain 0.0959487\n",
      "currEpoch 10 classErrTerm 0.052 errTrain 0.177016\n",
      "currEpoch 10 classErrTerm 0.034 errTrain 0.123866\n",
      "currEpoch 10 classErrTerm 0.04 errTrain 0.127173\n",
      "currEpoch 10 classErrTerm 0.032 errTrain 0.137427\n",
      "currEpoch 10 classErrTerm 0.04 errTrain 0.125793\n",
      "currEpoch 10 classErrTerm 0.038 errTrain 0.159964\n",
      "currEpoch 10 classErrTerm 0.026 errTrain 0.0952658\n",
      "currEpoch 10 classErrTerm 0.024 errTrain 0.0922117\n",
      "currEpoch 10 classErrTerm 0.034 errTrain 0.131078\n",
      "currEpoch 10 classErrTerm 0.02 errTrain 0.106394\n",
      "currEpoch 11\n",
      "currEpoch 12\n",
      "currEpoch 13\n",
      "currEpoch 14\n",
      "currEpoch 15\n",
      "currEpoch 15 classErrTerm 0.00999999 errTrain 0.0482894\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0744039\n",
      "currEpoch 15 classErrTerm 0.012 errTrain 0.0554567\n",
      "currEpoch 15 classErrTerm 0.014 errTrain 0.0818831\n",
      "currEpoch 15 classErrTerm 0.014 errTrain 0.0574019\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0593036\n",
      "currEpoch 15 classErrTerm 0.00999999 errTrain 0.0609653\n",
      "currEpoch 15 classErrTerm 0.022 errTrain 0.0807747\n",
      "currEpoch 15 classErrTerm 0.00599998 errTrain 0.0543318\n",
      "currEpoch 15 classErrTerm 0.012 errTrain 0.0615537\n",
      "currEpoch 15 classErrTerm 0.00999999 errTrain 0.0518414\n",
      "currEpoch 15 classErrTerm 0.014 errTrain 0.0548226\n",
      "currEpoch 15 classErrTerm 0.00800002 errTrain 0.061334\n",
      "currEpoch 15 classErrTerm 0.022 errTrain 0.0847772\n",
      "currEpoch 15 classErrTerm 0.014 errTrain 0.0634795\n",
      "currEpoch 15 classErrTerm 0.00800002 errTrain 0.0671602\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0628136\n",
      "currEpoch 15 classErrTerm 0.012 errTrain 0.0604398\n",
      "currEpoch 15 classErrTerm 0.018 errTrain 0.0679922\n",
      "currEpoch 15 classErrTerm 0.018 errTrain 0.0696003\n",
      "currEpoch 15 classErrTerm 0.014 errTrain 0.0507379\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0668045\n",
      "currEpoch 15 classErrTerm 0.00199997 errTrain 0.0358557\n",
      "currEpoch 15 classErrTerm 0.022 errTrain 0.071652\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0559908\n",
      "currEpoch 15 classErrTerm 0.00400001 errTrain 0.0430836\n",
      "currEpoch 15 classErrTerm 0.00999999 errTrain 0.0518129\n",
      "currEpoch 15 classErrTerm 0.00599998 errTrain 0.0488445\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0603645\n",
      "currEpoch 15 classErrTerm 0.016 errTrain 0.0630632\n",
      "currEpoch 16\n",
      "currEpoch 17\n",
      "currEpoch 18\n",
      "currEpoch 19\n",
      "$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\n",
      "(3, 20)\n",
      "$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\n",
      "(3, 20)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def hyperParamGen():\n",
    "    #random sampes using numpy\n",
    "    logLearningRate = np.random.uniform(-7.5, -4.5)\n",
    "    print(\"logLearningRate\", logLearningRate)\n",
    "    learningRate = np.exp(logLearningRate)\n",
    "    print(\"learningRate\", learningRate)\n",
    "    numLayers = np.random.random_integers(1, 5)\n",
    "    print(\"numLayers\", numLayers)\n",
    "    numHiddenUnits = np.random.random_integers(100, 500)\n",
    "    print(\"numHiddenUnits\", numHiddenUnits)\n",
    "    logWeightedDecay = np.random.uniform(-9, -6)\n",
    "    print(\"logWeightedDecay\", logWeightedDecay)\n",
    "    weightedDecay = np.exp(logWeightedDecay)\n",
    "    print(\"weightedDecay\", weightedDecay)\n",
    "    dropOut = np.random.random_integers(0, 1) # 0 or 1\n",
    "    print(\"dropOut\", dropOut)\n",
    "    \n",
    "    return learningRate, numLayers, numHiddenUnits, weightedDecay, dropOut\n",
    "\n",
    "def convertTarget(targetValues):\n",
    "    numClasses = np.max(targetValues) + 1\n",
    "    return np.eye(numClasses)[targetValues]\n",
    "\n",
    "class loadData:\n",
    "    def __init__(self):\n",
    "        self.flatten = True\n",
    "        self.addOnes = False\n",
    "        \n",
    "        self.data_path = '/Users/vikuo/Documents/GitHub/ece521/assi/A3/notMNIST.npz'\n",
    "    def arrFlatten(self, arr):\n",
    "        '''\n",
    "        type np array\n",
    "        '''\n",
    "        dataDim1, dum1, dum2 = arr.shape\n",
    "        dum_sq = dum1 * dum2\n",
    "        arr = np.reshape(arr, [ dataDim1 ,dum_sq ])\n",
    "        return arr   \n",
    "    '''\n",
    "    def convertTarget(self, targetValues):\n",
    "        numClasses = np.max(targetValues) + 1\n",
    "        return np.eye(numClasses)[targetValues]\n",
    "    '''\n",
    "    def loadNumData(self):\n",
    "        with np.load(self.data_path) as data:\n",
    "            Data, Target = data [\"images\"], data[\"labels\"]\n",
    "            np.random.seed(521)\n",
    "            randIndx = np.arange(len(Data))\n",
    "            np.random.shuffle(randIndx)\n",
    "            Data = Data[randIndx]/255.\n",
    "            \n",
    "            if self.flatten:\n",
    "                Data = self.arrFlatten(Data)\n",
    "            \n",
    "            Target = Target[randIndx]\n",
    "            trainData, trainTarget = Data[:15000], Target[:15000]\n",
    "            validData, validTarget = Data[15000:16000], Target[15000:16000]\n",
    "            testData, testTarget = Data[16000:], Target[16000:]\n",
    "            \n",
    "            trainTarget = convertTarget(trainTarget)\n",
    "            validTarget = convertTarget(validTarget)\n",
    "            testTarget = convertTarget(testTarget)\n",
    " \n",
    "        return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "        If m is given the batch will be of size m. \n",
    "        Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "\n",
    "class neuralNetwork:\n",
    "    # these are for testing only\n",
    "    def __init__(self, _learningRate = 0.05, _learningRateArr = [0.05], \n",
    "                 _numLayers = [1000], _weightedDecay = 3e-4, _dropOut = 0, _epochIter = 10000):\n",
    "        # learningRate, numLayers, numHiddenUnits, weightedDecay, dropOut\n",
    "        self.learningRate = _learningRate\n",
    "        self.learningRateArr = _learningRateArr\n",
    "        self.numLayers = _numLayers # number of units in each layer\n",
    "        self.numLayers.append(10)\n",
    "        print(\"init layer\", self.numLayers)\n",
    "        self.weightedDecay = _weightedDecay\n",
    "        self.dropOut = _dropOut\n",
    "        self.dropOutProb = 0.5\n",
    "        self.epochIter = _epochIter \n",
    "        \n",
    "        \n",
    "        # default, no arg taken\n",
    "        self.numPixel = 784 \n",
    "        self.numClass = 10\n",
    "        self.batchSize = 500\n",
    "        \n",
    "    def buildLayer(self, _inputTensor, _numUnits):\n",
    "        '''\n",
    "        input:\n",
    "            #feed# _inputTensor S = theta(Xprev)from the prev layer\n",
    "            _numUnits is the num of neurons in this layer\n",
    "    \n",
    "        intermediate xavierInit:\n",
    "            W is initialized as Xavier\n",
    "            W is input.shape[1] by num units\n",
    "    \n",
    "        output:\n",
    "        weighted sum of inputs\n",
    "    \n",
    "        '''\n",
    "        # zero mean independent Gaussians whose variance is 3/(#input + #outputs)  \n",
    "        dim1 = _inputTensor.get_shape().as_list()[1]\n",
    "        \n",
    "\n",
    "        \n",
    "        #Xavier\n",
    "        #W = tf.get_variable(\"W\", shape = [dim1, _numUnits], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        variance = 3.0 / (dim1 + _numUnits)\n",
    "        W = tf.Variable(tf.truncated_normal(shape = [dim1, _numUnits],  stddev = math.sqrt(variance)))\n",
    "        #b = tf.Variable(0.0, name='biases')\n",
    "        b = tf.Variable(tf.zeros([_numUnits]), name = 'biases')\n",
    "\n",
    "        \n",
    "        print(\"W\", dim1, _numUnits)\n",
    "        print(\"X shape\", _inputTensor.get_shape().as_list())\n",
    "        X = tf.matmul(tf.cast(_inputTensor, dtype = tf.float32), W) + b\n",
    "    \n",
    "        #X = tf.add(X, b)\n",
    "        print(\"X in layer\", X)\n",
    "        regTerm = tf.multiply( tf.constant(0.50, dtype = tf.float32),\\\n",
    "                        tf.multiply(tf.constant(self.weightedDecay,dtype = tf.float32), tf.reduce_mean(tf.square(W))))\n",
    "        \n",
    "        tf.add_to_collection(\"layerW\", W)\n",
    "        tf.add_to_collection(\"layerReg\", regTerm)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def accuracy(self, y_hat, target):\n",
    "        #TAKEN FROM last assignment\n",
    "        \n",
    "        target = tf.cast(target, dtype = tf.float32)\n",
    "        correctCases = tf.equal(tf.argmax(y_hat, 1), tf.argmax(target, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correctCases, dtype=\"float\"))\n",
    "        return accuracy #.eval()\n",
    "    \n",
    "    def classificationError(self, y_hat, target):\n",
    "        target = tf.cast(target, dtype = tf.float32)\n",
    "        correctCases = tf.equal(tf.argmax(y_hat, 1), tf.argmax(target, 1))\n",
    "        \n",
    "        error = 1 - tf.reduce_mean(tf.cast(correctCases, dtype=\"float\"))\n",
    "        return error\n",
    "\n",
    "    \n",
    "    def buildNet(self):\n",
    "        '''\n",
    "        input: \n",
    "            number of hidden units #in the class def\n",
    "            data #feed\n",
    "        output:\n",
    "            predicted labels\n",
    "        '''\n",
    "                \n",
    "        X = tf.placeholder(tf.float32, shape=[None, self.numPixel], name='dataX')\n",
    "        y_target = tf.placeholder(tf.float32, shape=[None, self.numClass], name='targetY')\n",
    "\n",
    "        X_prev = X #tf.convert_to_tensor(X)\n",
    "        for numUnits in self.numLayers: #numLayers is an array of num hidden units            \n",
    "            X_curr = self.buildLayer(X_prev, numUnits)\n",
    "            print(\"X_curr\", X_curr)\n",
    "            S_curr = tf.nn.relu(X_curr)\n",
    "            X_prev = S_curr\n",
    "            if self.dropOut:\n",
    "                X_prev = tf.nn.dropout(X_prev, self.dropOutProb)\n",
    "        \n",
    "        y_hat = X_curr\n",
    "        \n",
    "        crossEntropyErrorCurr = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits = y_hat, labels = y_target))\n",
    "        \n",
    "        regTermSum = sum(tf.get_collection(\"regTerm\"))\n",
    "        regTermSum = tf.cast(regTermSum, dtype = tf.float32)\n",
    "        print(\"regTermsum\", regTermSum)\n",
    "        print(\"crossEntropyError Curr\", crossEntropyErrorCurr)\n",
    "        \n",
    "        totalLoss = tf.add(regTermSum , crossEntropyErrorCurr)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
    "        train = optimizer.minimize(loss=totalLoss)\n",
    "        accuracyTerm = self.accuracy(y_hat, y_target)\n",
    "        classErrTerm = self.classificationError(y_hat, y_target)\n",
    "        W = tf.get_collection(\"layerW\")\n",
    "        \n",
    "        # return totalLoss also??\n",
    "        return W, X, y_hat, y_target, crossEntropyErrorCurr, accuracyTerm, classErrTerm, train   #final prediction\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def plotFig(self, _dim, y , addInfo, title=\"default\", xLabel=\"epoch\", yLabel=\"yLabel\", plotLabel =\"plotLabel\", _num =1 ):\n",
    "        x = np.linspace(0, _dim, num=_dim)\n",
    "        y = np.array(y)\n",
    "        print(\"$$$$$$$$$$$$$$$ in plot fig$$$$$$$$$$$$$$$$$\")\n",
    "        print(y.shape)\n",
    "        plt.figure(_num)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xLabel)\n",
    "        plt.ylabel(yLabel)\n",
    "        for i in range(y.shape[0]):\n",
    "            #print(y.shape[0])\n",
    "            #print(\"x\", x)\n",
    "            #print(\"y\", y[i])\n",
    "            plt.plot(x, y[i], label = plotLabel + str(addInfo[i]))\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig( title + str(_num) + \".png\")\n",
    "        plt.close()\n",
    "        plt.clf()\n",
    "        \n",
    "    def runPart1_1(self, trainData, trainTarget, validData, validTarget,testData, testTarget):\n",
    "        '''\n",
    "        input: data\n",
    "                1 learning rate to try\n",
    "                add another function to loop through other hyperparams\n",
    "        output:\n",
    "            classification error plot\n",
    "            cross entropy loss plot\n",
    "        '''\n",
    "        range_list = [0.25, 0.5, 0.75, 1] # save at progress %\n",
    "       \n",
    "        trainLossL = []\n",
    "        trainErrL = []\n",
    "        validErrL =[]\n",
    "        validLossL = []\n",
    "        testErrL = []\n",
    "        testLossL = []\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        W, X, y_hat, y_target, crossEntropyErrorCurr, accuracyTerm, classErrTerm, train = self.buildNet()\n",
    "            \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(init)\n",
    "        #initialW = sess.run(W)  \n",
    "        print(\"self.learningrate = \", self.learningRate)\n",
    "        trainBatchSampler = BatchSampler(trainData, trainTarget, self.batchSize)\n",
    "        print(\"self.epochIter\", self.epochIter)\n",
    "\n",
    "        for currEpoch in range(0, self.epochIter):\n",
    "            print(\"currEpoch\", currEpoch)\n",
    "            total_batch = int(trainData.shape[0]/self.batchSize)\n",
    "            for i in range(total_batch):\n",
    "                dataBatch, targetBatch = trainBatchSampler.get_batch()\n",
    "                currentW, entropyErrTrain, classErrTrain, y_predict, trainModel = \\\n",
    "                    sess.run([W, crossEntropyErrorCurr, classErrTerm, y_hat, train], feed_dict={X: dataBatch, y_target: targetBatch})\n",
    "                validLoss, validErr = sess.run([crossEntropyErrorCurr, classErrTerm],feed_dict = {X:validData, y_target: validTarget} )\n",
    "                testLoss, testErr = sess.run([crossEntropyErrorCurr, classErrTerm],feed_dict = {X: testData, y_target: testTarget})\n",
    "                                            \n",
    "                for j in range_list:\n",
    "                    if currEpoch == int(j * self.epochIter):\n",
    "                        print(\"currEpoch\", currEpoch ,\"classErrTerm\" , classErrTrain, \"errTrain\", entropyErrTrain)\n",
    "                        # plot at 25%, 50%, 75%, 100%\n",
    "                        #fileName = \n",
    "                        saver.save(sess, './my-model', global_step=i)\n",
    "                        \n",
    "            # indent for plotting vs iter\n",
    "            trainLossL.append(entropyErrTrain)\n",
    "            trainErrL.append(classErrTrain)\n",
    "            validLossL.append(validLoss)\n",
    "            validErrL.append(validErr)\n",
    "            testLossL.append(testLoss)\n",
    "            testErrL.append(testErr)\n",
    "                \n",
    "        #end for\n",
    "        #start plotting\n",
    "        allErr = [trainLossL] + [validLossL] + [testLossL]\n",
    "        \n",
    "        #print(\"allErr \", len(allErr))\n",
    "        #total_iter = len(trainLossL)\n",
    "        #self.plotFig(_dim = total_iter, y=allErr , addInfo = [self.learningRate], title=\"Q1_2ClassificationError vs iteration\", xLabel=\"iteration\", yLabel=\"Classification error\", plotLabel =\"learningRate\", _num =1 )\n",
    "        self.plotFig(_dim = self.epochIter, y = allErr, addInfo =[\"train\", \"valid\", \"test\"],\n",
    "                     title=\"Q1_2EntropyError vs epoch\", xLabel = \"epoch\",\n",
    "                     yLabel =\"Entropy error\", plotLabel =\"learningRate 0.001\", _num =1)\n",
    "        \n",
    "        allErr = [trainErrL] + [validErrL] + [testErrL]\n",
    "        self.plotFig(_dim = self.epochIter, y = allErr, addInfo =[\"train\", \"valid\", \"test\"],\n",
    "                     title=\"Q1_2ClassificationError vs epoch\", xLabel = \"epoch\",\n",
    "                     yLabel =\"Classification error\", plotLabel =\"learningRate 0.001\", _num =1)\n",
    "        \n",
    "def restoreSaver():\n",
    "    \n",
    "        restore_saver = tf.train.import_meta_graph('my_model-1000.meta')\n",
    "        restore_saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "        print(\"trying tf saver\")\n",
    "        print(sess.run(crossEntropyErrorCurr))\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "\n",
    "    dataLoader = loadData()\n",
    " \n",
    "    trainData, trainTarget, validData, validTarget,testData, testTarget = dataLoader.loadNumData()\n",
    "    \n",
    "    print(\"*******q1 part 1********\")\n",
    "    learningRates = [0.001]#[0.001, 0.001, 0.1] #for debugging accuracy\n",
    "    lambdaReg = 3e-4\n",
    "    # TODO: Plotting\n",
    "\n",
    "    singleLayerNet = neuralNetwork(_learningRate=learningRates[0], _epochIter = 20)\n",
    "\n",
    "    singleLayerNet.runPart1_1(trainData, trainTarget, validData, validTarget,testData, testTarget)\n",
    "    #restoreSaver()\n",
    "    \n",
    "\n",
    "    \n",
    "                      \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "a = [1]\n",
    "b = [2]\n",
    "c = [3]\n",
    "d = [a] + [b] +[c]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "* try other lambdas\n",
    "* restore value from meta files, plot for early stopping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
